{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7a9f8c",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026fe66",
   "metadata": {},
   "source": [
    "# Borradores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead97eed",
   "metadata": {},
   "source": [
    "## Obteniendo secuencias sobre la carpeta de genomas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ecdd8c",
   "metadata": {},
   "source": [
    "Importando librerias e iniciando SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c06873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, length\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda3b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/10 17:21:14 WARN Utils: Your hostname, Felipes-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.12 instead (on interface en0)\n",
      "25/07/10 17:21:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/10 17:21:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.appName(\"FASTA_Kmer_Matching\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d70e6e",
   "metadata": {},
   "source": [
    "Obtención de paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf2eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_obj = os.path.join(os.getcwd(), 'files', 'genome_ref')\n",
    "files_path = os.path.join(os.getcwd(), 'files', 'genomes')\n",
    "fasta_obj = os.path.join(path_obj, 'GCF_003119375.1_ASM311937v1_genomic.fna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b257df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCF_902702785.1_GD-0001_genomic.fna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCF_902702755.1_S10_genomic.fna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename\n",
       "0  GCF_902702785.1_GD-0001_genomic.fna\n",
       "1      GCF_902702755.1_S10_genomic.fna"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = pd.DataFrame(os.listdir(files_path), columns=['filename'])\n",
    "df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ce941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GCF_902702785.1_GD-0001_genomic.fna\n",
      "Processing file: GCF_902702755.1_S10_genomic.fna\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>seq_header</th>\n",
       "      <th>genome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCF_902702785.1_GD-0001_genomic.fna</td>\n",
       "      <td>&gt;NZ_LR738720.1 Streptococcus suis isolate GD-0...</td>\n",
       "      <td>ATGAACCAAGAACAACTTTTTTGGCAACGATTTATTGAATTGGCAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCF_902702755.1_S10_genomic.fna</td>\n",
       "      <td>&gt;NZ_LR738721.1 Streptococcus suis isolate S10 ...</td>\n",
       "      <td>ATGAACCAAGAACAACTTTTTTGGCAACGATTTATTGAATTGGCAA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename  \\\n",
       "0  GCF_902702785.1_GD-0001_genomic.fna   \n",
       "1      GCF_902702755.1_S10_genomic.fna   \n",
       "\n",
       "                                          seq_header  \\\n",
       "0  >NZ_LR738720.1 Streptococcus suis isolate GD-0...   \n",
       "1  >NZ_LR738721.1 Streptococcus suis isolate S10 ...   \n",
       "\n",
       "                                              genome  \n",
       "0  ATGAACCAAGAACAACTTTTTTGGCAACGATTTATTGAATTGGCAA...  \n",
       "1  ATGAACCAAGAACAACTTTTTTGGCAACGATTTATTGAATTGGCAA...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for file in df_path['filename']:\n",
    "    if not file.endswith('.fna'):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing file: {file}\")\n",
    "    file_path = os.path.join(files_path, file)\n",
    "    with open(file_path) as f:\n",
    "        header = f.readline().strip()\n",
    "        genome = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "    # Acumula en lista\n",
    "    records.append({\n",
    "        'filename': file,\n",
    "        'seq_header': header,\n",
    "        'genome': genome\n",
    "    })\n",
    "\n",
    "df_genome = pd.DataFrame(records)\n",
    "df_genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e940cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_rdd = sc.parallelize(df_genome['genome'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e8a30",
   "metadata": {},
   "source": [
    "## Obteniendo secuencias del archivo objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27df3408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGGAGGTTG'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(fasta_obj) as f:\n",
    "        header = f.readline().strip()\n",
    "        genome_obj = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "genome_obj[:10]  # primeros 100 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0c46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_obj_bc = sc.broadcast(genome_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61cefe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetro k\n",
    "k = 2\n",
    "# Función para obtener k-mers de longitud k\n",
    "def kmers(seq: str, k: int):\n",
    "    return [ seq[i : i + k] \n",
    "             for i in range(len(seq) - k + 1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fa4230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para contar hits de un k-mer en el genoma\n",
    "def find_kmers_in_genome(genome: str, kmer: str) -> list[int]:\n",
    "    positions = []\n",
    "    pos = genome.find(kmer)  # busca la 1ª aparición\n",
    "    while pos != -1:\n",
    "        positions.append(pos)  # guardamos la posición\n",
    "        pos = genome.find(kmer, pos + 1)  # buscamos la siguiente (permite solapamientos)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afeb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_read(record):\n",
    "    header, seq = record\n",
    "    kmer_list = kmers(seq, k)\n",
    "    t0 = time.time()\n",
    "    sum_cal = 0\n",
    "    all_pos = []\n",
    "    for mer in kmer_list:\n",
    "        positions = find_kmers_in_genome(genome_obj_bc.value, mer)\n",
    "        if positions:\n",
    "            sum_cal += len(positions)\n",
    "            all_pos.extend(positions)\n",
    "    dt = time.time() - t0\n",
    "    return {\"Sequence\": header, \"Sum_Cal\": sum_cal, \"Positions\": all_pos, \"Time(s)\": dt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b929e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar en paralelo y recolectar resultados\n",
    "rows = reads_rdd.map(process_read).collect()\n",
    "\n",
    "# Construir DataFrame y exportar\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv('scripts/results/output.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"El archivo output.csv ha sido creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b9c921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GCF_902702785.1_GD-0001_genomic.fna\n",
      "Processing file: GCF_902702755.1_S10_genomic.fna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/10 19:26:02 WARN TaskSetManager: Stage 1 contains a task of very large size (2079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "ERROR:root:KeyboardInterrupt while sending command.=====>         (10 + 2) / 12]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m genome_obj_bc \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(genome_obj)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Ejecutar en paralelo y recolectar resultados\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m results_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mreads_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_read\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Construir DataFrame y exportar\u001b[39;00m\n\u001b[1;32m     83\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results_rdd)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/core/rdd.py:1700\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py:1361\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/clientserver.py:535\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    536\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===============================================>         (10 + 2) / 12]\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, length\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.appName(\"FASTA_Kmer_Matching\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path_obj = os.path.join(os.getcwd(), 'files', 'genome_ref')\n",
    "files_path = os.path.join(os.getcwd(), 'files', 'genomes')\n",
    "fasta_obj = os.path.join(path_obj, 'GCF_003119375.1_ASM311937v1_genomic.fna')\n",
    "\n",
    "df_path = pd.DataFrame(os.listdir(files_path), columns=['filename'])\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in df_path['filename']:\n",
    "    if not file.endswith('.fna'):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing file: {file}\")\n",
    "    file_path = os.path.join(files_path, file)\n",
    "    with open(file_path) as f:\n",
    "        header = f.readline().strip()\n",
    "        genome = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "    # Acumula en lista\n",
    "    records.append({\n",
    "        'filename': file,\n",
    "        'seq_header': header,\n",
    "        'genome': genome\n",
    "    })\n",
    "\n",
    "with open(fasta_obj) as f:\n",
    "        header = f.readline().strip()\n",
    "        genome_obj = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "# Tamaño de k-mers\n",
    "k = 2\n",
    "def kmers(seq: str, k: int):\n",
    "    return [ seq[i : i + k] \n",
    "             for i in range(len(seq) - k + 1) ]\n",
    "\n",
    "# Función para contar hits de un k-mer en el genoma\n",
    "def find_kmers_in_genome(genome: str, kmer: str) -> list[int]:\n",
    "    positions = []\n",
    "    pos = genome.find(kmer)  # busca la 1ª aparición\n",
    "    while pos != -1:\n",
    "        positions.append(pos)  # guardamos la posición\n",
    "        pos = genome.find(kmer, pos + 1)  # buscamos la siguiente (permite solapamientos)\n",
    "    return positions\n",
    "\n",
    "def process_read(seq: str):\n",
    "    kmer_list = kmers(seq, k)\n",
    "    t0 = time.time()\n",
    "    sum_cal = 0\n",
    "    all_pos = []\n",
    "    for mer in kmer_list:\n",
    "        positions = find_kmers_in_genome(genome_obj_bc.value, mer)\n",
    "        if positions:\n",
    "            sum_cal += len(positions)\n",
    "            all_pos.extend(positions)\n",
    "    dt = time.time() - t0\n",
    "    return {\"Sequence\": seq[:30] + \"...\", \"Sum_Cal\": sum_cal, \"Positions\": all_pos, \"Time(s)\": dt}\n",
    "\n",
    "# Crear DataFrame de Pandas con los registros\n",
    "df_genome = pd.DataFrame(records)\n",
    "reads_rdd = sc.parallelize(df_genome['genome'].tolist())\n",
    "\n",
    "# Broadcast del genoma de referencia\n",
    "genome_obj_bc = sc.broadcast(genome_obj)\n",
    "\n",
    "\n",
    "# Ejecutar en paralelo y recolectar resultados\n",
    "results_rdd = reads_rdd.map(process_read).collect()\n",
    "\n",
    "# Construir DataFrame y exportar\n",
    "df = pd.DataFrame(results_rdd)\n",
    "df.to_csv('scripts/results/output.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"El archivo output.csv ha sido creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f41589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GCF_902702785.1_GD-0001_genomic.fna\n",
      "Processing file: GCF_902702755.1_S10_genomic.fna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/10 19:31:14 WARN TaskSetManager: Stage 2 contains a task of very large size (2079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:==============>  (10 + 2) / 12][Stage 3:>                  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, length\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.appName(\"FASTA_Kmer_Matching\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path_obj = os.path.join(os.getcwd(), 'files', 'genome_ref')\n",
    "files_path = os.path.join(os.getcwd(), 'files', 'genomes')\n",
    "fasta_obj = os.path.join(path_obj, 'GCF_003119375.1_ASM311937v1_genomic.fna')\n",
    "\n",
    "df_path = pd.DataFrame(os.listdir(files_path), columns=['filename'])\n",
    "\n",
    "records = []\n",
    "\n",
    "for file in df_path['filename']:\n",
    "    if not file.endswith('.fna'):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing file: {file}\")\n",
    "    file_path = os.path.join(files_path, file)\n",
    "    with open(file_path) as f:\n",
    "        header = f.readline().strip()\n",
    "        genome = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "    # Acumula en lista\n",
    "    records.append({\n",
    "        'filename': file,\n",
    "        'seq_header': header,\n",
    "        'genome': genome\n",
    "    })\n",
    "\n",
    "with open(fasta_obj) as f:\n",
    "        header = f.readline().strip()\n",
    "        genome_obj = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "# Tamaño de k-mers\n",
    "k = 2\n",
    "def kmers(seq: str, k: int):\n",
    "    return [ seq[i : i + k] \n",
    "             for i in range(len(seq) - k + 1) ]\n",
    "\n",
    "def build_kmer_index(genome: str, k: int):\n",
    "    index = {}\n",
    "    for i in range(len(genome) - k + 1):\n",
    "        kmer = genome[i:i+k]\n",
    "        index.setdefault(kmer, []).append(i)\n",
    "    return index\n",
    "\n",
    "def process_read(seq: str):\n",
    "    kmer_list = kmers(seq, k)\n",
    "    t0 = time.time()\n",
    "    sum_cal = 0\n",
    "    all_pos = []\n",
    "    for mer in kmer_list:\n",
    "        positions = genome_index_bc.value.get(mer, [])\n",
    "        if positions:\n",
    "            sum_cal += len(positions)\n",
    "            all_pos.extend(positions)\n",
    "    dt = time.time() - t0\n",
    "    return {\"Sequence\": seq[:30] + \"...\", \"Sum_Cal\": sum_cal, \"Positions\": all_pos, \"Time(s)\": dt}\n",
    "\n",
    "# Crear DataFrame de Pandas con los registros\n",
    "df_genome = pd.DataFrame(records)\n",
    "reads_rdd = sc.parallelize(df_genome['genome'].tolist())\n",
    "\n",
    "genome_index = build_kmer_index(genome_obj, k)\n",
    "genome_index_bc = sc.broadcast(genome_index)\n",
    "\n",
    "reads_rdd = reads_rdd.repartition(8)  # ajusta el número a los núcleos disponibles\n",
    "\n",
    "df_results = spark.createDataFrame(reads_rdd.map(process_read))\n",
    "df_results.toPandas().to_csv('scripts/results/output.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"El archivo output.csv ha sido creado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4debc53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/10 20:47:22 WARN Utils: Your hostname, Felipes-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.10 instead (on interface en8)\n",
      "25/07/10 20:47:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/10 20:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:root:Exception while sending command.==========>              (3 + 1) / 4]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=76>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/clientserver.py\", line 566, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/core/context.py\", line 389, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/core/context.py\", line 2487, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1362, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 282, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/protocol.py\", line 335, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o15.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/felipeespinoza/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/clientserver.py\", line 566, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute results in parallel and write out\u001b[39;00m\n\u001b[1;32m     67\u001b[0m rows_rdd \u001b[38;5;241m=\u001b[39m reads_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m rec: process_read(rec[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rec, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m process_read(rec))\n\u001b[0;32m---> 68\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \\\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/output_csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults written to results/output_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/sql/session.py:1599\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pyarrow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pa\u001b[38;5;241m.\u001b[39mTable):\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from PyArrow Table.\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1597\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1598\u001b[0m     )\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/sql/session.py:1641\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m-> 1641\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1643\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;28mmap\u001b[39m(prepare, data), schema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/sql/session.py:1161\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1161\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1163\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/sql/session.py:1098\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1079\u001b[0m     rdd: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Any]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1080\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1081\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1082\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(first) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   1101\u001b[0m             errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1102\u001b[0m             messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   1103\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/core/rdd.py:2755\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2731\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2732\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2754\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2755\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/core/rdd.py:2722\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2719\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2721\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2722\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2724\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2725\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/core/context.py:2551\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2549\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2551\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/py4j/protocol.py:335\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    337\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.appName(\"FASTA_Kmer_Matching\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path_obj = os.path.join(os.getcwd(), 'files', 'genome_ref')\n",
    "files_path = os.path.join(os.getcwd(), 'files', 'genomes')\n",
    "fasta_obj = os.path.join(path_obj, 'GCF_003119375.1_ASM311937v1_genomic.fna')\n",
    "\n",
    "# Read all FASTA files in parallel and parse into (header, sequence) tuples\n",
    "files_rdd = sc.wholeTextFiles(os.path.join(files_path, '*.fna'))\n",
    "\n",
    "def parse_fasta(file_pair):\n",
    "    _, text = file_pair\n",
    "    recs = []\n",
    "    for record in text.split('>'):\n",
    "        if not record: continue\n",
    "        lines = record.splitlines()\n",
    "        header = lines[0]\n",
    "        seq = ''.join(lines[1:])\n",
    "        recs.append((header, seq))\n",
    "    return recs\n",
    "\n",
    "reads_rdd = files_rdd.flatMap(parse_fasta) \\\n",
    "                     .repartition(8)\n",
    "\n",
    "with open(fasta_obj) as f:\n",
    "    header = f.readline().strip()\n",
    "    genome_obj = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "# Tamaño de k-mers\n",
    "k = 2\n",
    "def kmers(seq: str, k: int):\n",
    "    return [ seq[i : i + k] \n",
    "             for i in range(len(seq) - k + 1) ]\n",
    "\n",
    "def build_kmer_index(genome: str, k: int):\n",
    "    index = {}\n",
    "    for i in range(len(genome) - k + 1):\n",
    "        kmer = genome[i:i+k]\n",
    "        index.setdefault(kmer, []).append(i)\n",
    "    return index\n",
    "\n",
    "def process_read(seq: str):\n",
    "    kmer_list = kmers(seq, k)\n",
    "    t0 = time.time()\n",
    "    sum_cal = 0\n",
    "    all_pos = []\n",
    "    for mer in kmer_list:\n",
    "        positions = genome_index_bc.value.get(mer, [])\n",
    "        if positions:\n",
    "            sum_cal += len(positions)\n",
    "            all_pos.extend(positions)\n",
    "    dt = time.time() - t0\n",
    "    return {\"Sequence\": seq[:30] + \"...\", \"Sum_Cal\": sum_cal, \"Positions\": all_pos, \"Time(s)\": dt}\n",
    "\n",
    "# Build genome index and broadcast\n",
    "genome_index = build_kmer_index(genome_obj, k)\n",
    "genome_index_bc = sc.broadcast(genome_index)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Compute results in parallel and write out\n",
    "rows_rdd = reads_rdd.map(lambda rec: process_read(rec[1]) if isinstance(rec, tuple) else process_read(rec))\n",
    "spark_df = spark.createDataFrame(rows_rdd.map(lambda d: Row(**d)))\n",
    "spark_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv('results/output_csv')\n",
    "\n",
    "print(\"Results written to results/output_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63be90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:======================================================> (53 + 1) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results written to results/output_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.appName(\"FASTA_Kmer_Matching\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path_obj = os.path.join(os.getcwd(), 'files', 'genome_ref')\n",
    "files_path = os.path.join(os.getcwd(), 'files', 'genomes')\n",
    "fasta_obj = os.path.join(path_obj, 'GCF_003119375.1_ASM311937v1_genomic.fna')\n",
    "\n",
    "# Read all FASTA files in parallel and parse into (header, sequence) tuples\n",
    "files_rdd = sc.wholeTextFiles(os.path.join(files_path, '*.fna'))\n",
    "\n",
    "def parse_fasta(file_pair):\n",
    "    _, text = file_pair\n",
    "    recs = []\n",
    "    for record in text.split('>'):\n",
    "        if not record: continue\n",
    "        lines = record.splitlines()\n",
    "        header = lines[0]\n",
    "        seq = ''.join(lines[1:])\n",
    "        recs.append((header, seq))\n",
    "    return recs\n",
    "\n",
    "reads_rdd = files_rdd.flatMap(parse_fasta) \\\n",
    "                     .repartition(54)\n",
    "\n",
    "with open(fasta_obj) as f:\n",
    "    header = f.readline().strip()\n",
    "    genome_obj = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "# Tamaño de k-mers\n",
    "k = 2\n",
    "def kmers(seq: str, k: int):\n",
    "    return [ seq[i : i + k] \n",
    "             for i in range(len(seq) - k + 1) ]\n",
    "\n",
    "def build_kmer_index(genome: str, k: int):\n",
    "    index = {}\n",
    "    for i in range(len(genome) - k + 1):\n",
    "        kmer = genome[i:i+k]\n",
    "        index.setdefault(kmer, []).append(i)\n",
    "    return index\n",
    "\n",
    "def process_read(seq: str):\n",
    "    kmer_list = kmers(seq, k)\n",
    "    t0 = time.time()\n",
    "    sum_cal = 0\n",
    "    all_pos = []\n",
    "    for mer in kmer_list:\n",
    "        sum_cal += len(genome_index_bc.value.get(mer, []))\n",
    "    dt = time.time() - t0\n",
    "    return {\"Sequence\": seq[:30] + \"...\",\n",
    "             \"Sum_Cal\": sum_cal,\n",
    "               \"Time(s)\": dt}\n",
    "\n",
    "# Build genome index and broadcast\n",
    "genome_index = build_kmer_index(genome_obj, k)\n",
    "genome_index_bc = sc.broadcast(genome_index)\n",
    "\n",
    "# Compute results in parallel and write out\n",
    "rows_rdd = reads_rdd.map(lambda rec: process_read(rec[1]) if isinstance(rec, tuple) else process_read(rec))\n",
    "spark_df = spark.createDataFrame(rows_rdd.map(lambda d: Row(**d)))\n",
    "spark_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv('results/output_csv')\n",
    "\n",
    "print(\"Results written to results/output_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4520833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTA summaries written to Results/summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-mer match summaries written to Results/summary_k2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:======================================================> (53 + 1) / 54]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results written to results/output_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder.appName(\"FASTA_Kmer_Matching\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path_obj = os.path.join(os.getcwd(), 'files', 'genome_ref')\n",
    "files_path = os.path.join(os.getcwd(), 'files', 'genomes')\n",
    "fasta_obj = os.path.join(path_obj, 'GCF_003119375.1_ASM311937v1_genomic.fna')\n",
    "\n",
    "# Read all FASTA files in parallel and parse into (header, sequence) tuples\n",
    "files_rdd = sc.wholeTextFiles(os.path.join(files_path, '*.fna'))\n",
    "\n",
    "# Summarize each FASTA file: count sequences, total length, GC content\n",
    "def summarize_fasta(file_pair):\n",
    "    path, text = file_pair\n",
    "    lines = text.splitlines()\n",
    "    num_seq = sum(1 for line in lines if line.startswith('>'))\n",
    "    seq_lines = [line.strip() for line in lines if not line.startswith('>')]\n",
    "    total_len = sum(len(seq) for seq in seq_lines)\n",
    "    gc_count = sum(seq.count('G') + seq.count('C') for seq in seq_lines)\n",
    "    gc_pct = round(100 * gc_count / total_len, 2) if total_len > 0 else 0.0\n",
    "    filename = os.path.basename(path)\n",
    "    return Row(filename=filename,\n",
    "               num_sequences=num_seq,\n",
    "               total_length=total_len,\n",
    "               gc_pct=gc_pct)\n",
    "\n",
    "# Create Results directory if not exists\n",
    "os.makedirs(\"Results\", exist_ok=True)\n",
    "# Create DataFrame of summaries and write CSV\n",
    "summary_df = spark.createDataFrame(files_rdd.map(summarize_fasta))\n",
    "summary_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"Results/summary\")\n",
    "print(\"FASTA summaries written to Results/summary\")\n",
    "\n",
    "def parse_fasta(file_pair):\n",
    "    _, text = file_pair\n",
    "    recs = []\n",
    "    for record in text.split('>'):\n",
    "        if not record: continue\n",
    "        lines = record.splitlines()\n",
    "        header = lines[0]\n",
    "        seq = ''.join(lines[1:])\n",
    "        recs.append((header, seq))\n",
    "    return recs\n",
    "\n",
    "reads_rdd = files_rdd.flatMap(parse_fasta) \\\n",
    "                     .repartition(54)\n",
    "\n",
    "with open(fasta_obj) as f:\n",
    "    header = f.readline().strip()\n",
    "    genome_obj = \"\".join(line.strip() for line in f if not line.startswith(\">\"))\n",
    "\n",
    "# Tamaño de k-mers\n",
    "k = 2\n",
    "def kmers(seq: str, k: int):\n",
    "    return [ seq[i : i + k] \n",
    "             for i in range(len(seq) - k + 1) ]\n",
    "\n",
    "def build_kmer_index(genome: str, k: int):\n",
    "    index = {}\n",
    "    for i in range(len(genome) - k + 1):\n",
    "        kmer = genome[i:i+k]\n",
    "        index.setdefault(kmer, []).append(i)\n",
    "    return index\n",
    "\n",
    "def process_read(seq: str):\n",
    "    kmer_list = kmers(seq, k)\n",
    "    t0 = time.time()\n",
    "    sum_cal = 0\n",
    "    all_pos = []\n",
    "    for mer in kmer_list:\n",
    "        sum_cal += len(genome_index_bc.value.get(mer, []))\n",
    "    dt = time.time() - t0\n",
    "    return {\"Sequence\": seq[:30] + \"...\",\n",
    "             \"Sum_Cal\": sum_cal,\n",
    "               \"Time(s)\": dt}\n",
    "\n",
    "# Build genome index and broadcast\n",
    "genome_index = build_kmer_index(genome_obj, k)\n",
    "genome_index_bc = sc.broadcast(genome_index)\n",
    "\n",
    "# Summarize each FASTA for k=2 matches in the reference\n",
    "def summarize_k2(file_pair):\n",
    "    path, text = file_pair\n",
    "    lines = text.splitlines()\n",
    "    num_seq = sum(1 for line in lines if line.startswith('>'))\n",
    "    seq_lines = [line.strip() for line in lines if not line.startswith('>')]\n",
    "    total_len = sum(len(s) for s in seq_lines)\n",
    "    seq_str = \"\".join(seq_lines)\n",
    "    # Count all 2-mer matches against the reference index\n",
    "    k2_count = 0\n",
    "    for i in range(len(seq_str) - 1):\n",
    "        k2 = seq_str[i:i+2]\n",
    "        k2_count += len(genome_index_bc.value.get(k2, []))\n",
    "    filename = os.path.basename(path)\n",
    "    return Row(filename=filename,\n",
    "               num_sequences=num_seq,\n",
    "               total_length=total_len,\n",
    "               k2_matches=k2_count)\n",
    "\n",
    "# Write summary of 2-mer matches\n",
    "os.makedirs(\"Results\", exist_ok=True)\n",
    "summary_k2_df = spark.createDataFrame(files_rdd.map(summarize_k2))\n",
    "summary_k2_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"Results/summary_k2\")\n",
    "print(\"2-mer match summaries written to Results/summary_k2\")\n",
    "\n",
    "# Compute results in parallel and write out\n",
    "rows_rdd = reads_rdd.map(lambda rec: process_read(rec[1]) if isinstance(rec, tuple) else process_read(rec))\n",
    "spark_df = spark.createDataFrame(rows_rdd.map(lambda d: Row(**d)))\n",
    "spark_df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv('results/output_csv')\n",
    "\n",
    "print(\"Results written to results/output_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9d691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f23a5f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
